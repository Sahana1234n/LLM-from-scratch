{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e363cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":1024,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2221422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self , d_in , d_out , context_length , n_heads , dropout , qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % n_heads == 0), \\\n",
    "        \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_out // n_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in , d_out , bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in , d_out , bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in , d_out , bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out , d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n",
    "        )\n",
    "\n",
    "        def forward(self , x):\n",
    "           b , num_tokens , d_in = x.shape\n",
    "\n",
    "           keys = self.W_key(x)\n",
    "           queries = self.W_query(x)\n",
    "           values = self.W_values(x)\n",
    "\n",
    "           # we implicitly split the matrix by adding a n_heads dimension\n",
    "           #unroll last dim: (b , num_tokens , d_out) -> (b,num_tokens , n_heads , head_dim)\n",
    "           \n",
    "           keys = keys.view(b , num_tokens , self.n_heads , self.head_dim)\n",
    "           queries = queries.view(b , num_tokens , self.n_heads , self.head_dim)\n",
    "           values = values.view(b , num_tokens , self.n_heads , self.head_dim)\n",
    "\n",
    "           # Transpose: (b , num_tokens , n_heads , head_dim)-> (b, n_heads, num_tokens, head_dim)\n",
    "           keys = keys.transpose(1,2)\n",
    "           queries = queries.transpose(1,2)\n",
    "           values = values.transpose(1,2)\n",
    "\n",
    "           #compute scaled dot-product attention \n",
    "\n",
    "           attention_scores = queries@keys.transpose(2,3)  #(b , n_heads , num_tokens , num_tokens)\n",
    "\n",
    "           # original mask truncated to the number of tokens and converted to boolean\n",
    "\n",
    "           mask_bool = self.mask.bool()[:num_tokens , :num_tokens]\n",
    "\n",
    "           # use the mask to fill attention scores\n",
    "           attention_scores.masked_fill(mask_bool , -torch.inf)\n",
    "\n",
    "           attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5 , dim=-1)\n",
    "\n",
    "           attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "           # shape: (b , num_tokens , n_heads , head_dim)\n",
    "\n",
    "           context_vec = (attention_weights@values).transpose(1,2)\n",
    "\n",
    "           #combine heads , where d_out = self.n_heads * self.head_dim\n",
    "           context_vec = context_vec.contiguous().view(b , num_tokens , self.d_out)\n",
    "           context_vec = self.out_proj(context_vec)\n",
    "\n",
    "           return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self , emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps) \n",
    "        return self.scale*norm_x+self.shift\n",
    "\n",
    "    def GELU(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self , x):\n",
    "            return 0.5 * x * (1+torch(\n",
    "                torch.sqrt(torch.tensor(2.0/torch.pi))*\n",
    "                (x+0.044715*torch.pow(x,3))\n",
    "            ))     \n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "        def __init__(self , cfg):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(cfg['emb_dim'] , 4*cfg['emb_dim']),\n",
    "                GELU(),\n",
    "                nn.Linear(4*cfg['emb_dim'],cfg['emb_dim']),\n",
    "            )    \n",
    "\n",
    "        def forward(self,x):\n",
    "            return self.layers(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9580d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranformerBlock(nn.Module):\n",
    "    def __init__(self , cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg['n_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias']\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self , x):\n",
    "        shortcut = x\n",
    "        x =self.norm1(x)\n",
    "        x = self.att(x) \n",
    "        x = x+shortcut   \n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
